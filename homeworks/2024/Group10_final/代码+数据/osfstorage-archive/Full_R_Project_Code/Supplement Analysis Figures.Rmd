--- 
title: "Supplementary Information, Code, and Plots"
subtitle: "Testing Theory of Mind in GPT Models and Humans"
output: 
 html_document:
  toc: yes
  code_folding: hide
  toc_float: 
   collapsed: false
  df_print: kable
  highlight: tango
---

<!-- This project has been distributed using a self-contained `renv` environment. 
# In order to run this file, it is important to ensure that the generated auto-
# loaders `.Rprofile` and `renv/activate.R` are included in the download and 
# are stored in the same project folder
# When first launching this project, `renv` should automatically bootstrap 
# itself, downloading and installing all of the necessary files and versions 
# into the project library. After this has been completed, make sure to run the 
# following command to restore the project locally on your machine:
#   	renv::restore()
# Press Y when prompted to activate the project to ensure that `renv::restore()` 
# restores packages into the project library as expected
# This R project and all constituent code has been written and compiled by 
# James Strachan [james.wa.strachan@gmail.com]
-->

```{r setup, include=FALSE, cache=FALSE}
options(scipen = 1, digits = 2) # set output to two decimal places 

# suppress warning messages in Markdown output
options(warn = -1)
options(dplyr.summarise.inform = FALSE)
```

This Markdown file contains the code for analysing and plotting the data reported in Strachan et al. (2023) _Testing Theory of Mind in GPT Models and Humans_. It also contains supplementary materials and analyses referenced in the main text. This is packaged as part of an R project within a reproducible environment consisting of all packages and dependencies required to run the code. All information about the R environment is available in the file `renv.lock` The GPT data on the full battery reported in the main manuscript and in the supplementary material were collected between 3 April and 18 April 2023. The follow-up data using an adapted version of the Faux Pas test were collected between 28 April and 4 May 2023. The follow-up data with GPT-3.5 using a randomised presentation order on the Irony, Strange Stories, and Faux Pas tests were collected between 24 April and 18 May 2023. Three LLaMA2-Chat models were tested between October and November 2023. Variant testing of the False Belief and Faux Pas tests for GPT models occurred between 25 October and 3 November 2023. 

_*Note:* Where individual text responses are reported, they are identified using an alphanumeric code to facilitate cross-checking with the published dataset, consisting of a 2-letter code for task (HT: Hinting; FA/FB: False Belief (counterbalancing list A or B); FP: Faux Pas; SS: Strange Stories; IA/IB: Irony comprehension (counterbalancing list A or B)), the model generating the response (3: GPT-3.5; 4: GPT-4; L: LLaMA2-70B; H: Human), the session or participant number (1-15 for LLMs; 1-51 for humans), and the index of the item within the test. e.g. HT-3.2.5 refers to GPT-3.5's response to the fifth item on the Hinting Task in the second testing session._

# Results {.tabset .unnumbered}

## Global settings and plot aesthetics {.unnumbered}
<h2>Global Settings</h2>

Here, the global settings and plot aesthetics are defined for use when plotting

Load libraries and main data files

```{r, warning=FALSE, message=FALSE}

# load required libraries
library(dplyr) #
library(tidyr) #  }- general user functions
library(Hmisc) #
library(purrr) #
library(ggplot2)    # plotting figures
library(ggpubr)     # tools for improving the plot aesthetics
library(simr)       # power calculations for glm 
library(ggdist)     # plotting distributions 
library(grid)       # for arranging grobs
library(cowplot)    # for getting figure legends
library(rstatix)    # for adding planned comparisons to plots
library(kableExtra) # for tables
library(flextable)  # also for tables
library(DescTools)  # for binomial MLE CIs

# load in main data
df_file <- read.csv(file = 'scored_data/scores_gpt.csv', header = T)
df_human <- read.csv(file = 'scored_data/scores_human.csv', header = T)

df <- df_file %>% filter(! trial_state %in% c('FC','TC'))   # remove two False Belief trials that are not part of the current study
dfh <- df_human %>% filter(! trial_state %in% c('FC','TC'))   # remove two False Belief trials that are not part of the current study
```

User-defined functions

```{r}
# function to generate a gradually darkening colour series between white and a specified base colour
#   base:   base colour to use as primary end of the series (e.g. 'red')
#   white:  second colour for the other end of the series. Default white but can be changed
#   n:      number of colours to generate (e.g. n=3 generates 3 colours equally spaced varying from pale to intense)
#   rev:    boolean to determine whether the colours go in ascending (F: default) or descending (T) order of intensity
createColourValues <- function(base, white = 'white', n = 4, rev = F) {
  # generate a colour range between the two colours
  colour_range <- ifelse(rev, colorRampPalette(c(white,base)), colorRampPalette(c(base,white)))
  
  # generate colours from the colour range 
  colours <- colour_range(n+2)
  colours <- colours[1:n+1]
  
  return(colours)
}
```

Global plot aesthetics 

```{r}
# define the colour palette 
cp <- data.frame(
  ChatGPT = '#48b5c4',
  GPT4 = '#115f9a',
  LLaMA = '#43b768',
  Human = '#9158b5',
  success = 'darkgreen',
  mixed = 'goldenrod',
  failure = 'firebrick',
  FB_False = '#fe7fd2',
  FB_True = '#7f3d69',
  StrangeStories = 'black',
  fauxpas = '#d7658b',
  neutral = '#dedad2',
  knowImp = '#54bebe'
)

# set global theme to theme from ggpubr::
theme_set(theme_pubr())

# set spacing
sig_spacing <- 0.05  # vertical spacing of significance markers
dodge_spacing <- 0.7 # dodge spacing of grouped data 

# set sizes
fs <- 10       # font size
fs_s <- fs-1  # font size (smaller)
ps <- 1       # point size
lw <- 0.5     # line width

ps_l <- ps*2  # large point size
lw_l <- lw*2  # large line width 

text <- element_text(size = fs, family = 'sans')
axis_text <- element_text(size = fs_s, family = 'sans')

# define the global theme details
th <- theme(
  legend.position = 'top',
  legend.title = element_blank(),
  axis.text.x = axis_text,
  axis.text.y = axis_text,
  axis.title.x = text,
  axis.title.y = text,
  strip.text.x = element_text(size = fs, family = 'sans', face = 'bold'),
  strip.text.y = element_text(size = fs, family = 'sans', face = 'bold'),
  strip.background = element_blank()
) 

sc <- scale_colour_manual(values = c(cp$GPT4, cp$ChatGPT, cp$LLaMA)) 
sf <- scale_fill_manual(values = c(cp$GPT4, cp$ChatGPT, cp$LLaMA))

# Method for multiple comparisons correction
mccor <- 'holm'
```

```{css, include=F}
.tabset h2 {display: none;}    /* don't show headers */ 
```

## Main Manuscript {.unnumbered}
<h2>Main Manuscript</h2>
Here you will find the code for all analysis and plots included in the main manuscript. Note that this only generates individual panels - the code for arranging and saving the final figures is included in the `Generate figures` tab

### Performance Across Theory of Mind tests 
Goal: 

* Collate data for GPT and humans into standardised scores for each session/participant for each Theory of Mind test, and combine them into a single data frame

* Compare performance of each LLM against human baseline in each condition 

* Plot confidence intervals for each condition [Barbell plot]

* Compare performance on original vs. new items to control for training set inclusion

* Plot individual sessions/participants to allow for visual comparison of human and GPT on each test [Violin plot]

Notes:

* Once newly generated items are tested as a control, subsequent analysis focusses only on original, validated items

* No distinction between item types in False Belief or Irony tasks (i.e. False/True; Ironic/Non-ironic)

```{r}
# create a data frame of the GPT data
df1 <- df %>%
  #filter(source == 'old') %>%     # get old items
  mutate(                          
    task = apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])),
    source = factor(source, levels = c('old','new'), labels = c('Original', 'New'))) %>% 
  gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
  mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>%
  filter(!is.na(score)) %>%
  group_by(task, trial, source, model) %>%
  summarise(score = mean(score, na.rm = T))

# the same data frame for the human data 
df1h <- dfh %>%
    #filter(source == 'old') %>%     # get old items
    mutate(                         
      task = apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])),
      source = factor(source, levels = c('old','new'), labels = c('Original', 'New'))) %>% 
    gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
    mutate(trial = factor(trial, levels = sprintf('score%s',c(1:51)))) %>%
    filter(!is.na(score)) %>% mutate(score = as.numeric(score)) %>% # inclusion of NA characters in the human data make it worth double-checking
    group_by(task, trial, source, model) %>%
    summarise(score = mean(score, na.rm = T))

# for plotting and analysis, we want to have some sensible order to the presentation of the tests
# for that, the best option seems to be to rank them in terms of how well humans did, so we have a sense of what a human-like pattern of responses would be 
order <- df1h %>% 
  group_by(task) %>% 
  summarise(mean = mean(score)) %>% 
  ungroup() %>%
  arrange(., -mean) %>% 
  mutate(task_narrow = ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)))

# Collate everything into a single data frame
df1 <- rbind(df1,df1h) 
df1$model <- factor(df1$model, levels = c('Human','GPT-4','GPT-3.5','LLaMA-70B'), labels =c('Human','GPT-4','GPT-3.5','LLaMA2-70B'))
df1$task <- factor(df1$task, levels = order$task)
```

#### Analysis - Human vs. LLM pairwise tests 

The next step is to compare LLMs' performance against human baseline in each condition with a set of two-way Wilcoxon tests. Correction for multiple comparisons is defined in the `Global settings` as a Holm correction. False Belief scores are not compared as performance was at ceiling without enough variance to apply statistical tests.

```{r}
# by-task model comparison 
comps <- df1 %>% 
  filter(source == 'Original') %>% 
    group_by(task, trial, model) %>%
    summarise(score = mean(score, na.rm = T)) %>% 
    filter(task != 'False Belief') %>%   # 100% accuracy in most conditions makes it impossible to compare
    ungroup() %>% 
  rowwise() %>%
  mutate(task = factor(ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)))) %>%
  ungroup() %>%
  group_by(task) %>%
  wilcox_test(score ~ model, detailed = T) %>% 
  filter(group1 == 'Human') %>% 
  adjust_pvalue(method = mccor) %>% 
  mutate(p.adj.signif = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '\\*','')))) %>% 
  select(-.y., -method, -alternative, -p, -n1, -n2)

comps %>% kable() %>% kable_styling()
```

#### Figure 1A: Violin plot
Violin plot of proportional scores on original test items for each test showing the distribution of test scores for individual sessions and participants. Coloured points show the proportional score (the average of all items on a given test) for a single test session (GPT) or participant (Humans) on original items. Black points indicate the mean for each condition. 

```{r, out.width='100%'}

f1A_violin <- df1 %>% 
    filter(source == 'Original') %>% 
    rowwise() %>% 
    mutate(task = factor(ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)), levels = order$task_narrow)) %>%
    ungroup() %>% 
    mutate(
        psig = factor(apply(., 1, function(x) ifelse(x['model'] == 'Human', NA, comps$p.adj.signif[comps$group2 == x['model'] & comps$task == x['task']])),
                      levels = c('***','**','\\*',''),
                      labels = c('***','**','*','')),
        ymax = ifelse(
            model == 'GPT-4', 1 + sig_spacing, ifelse(
                model == 'GPT-3.5', 1 + 2*sig_spacing, ifelse(
                    model == 'LLaMA2-70B', 1 + 3*sig_spacing, NA))),
        xdodge = ifelse(
            model == 'GPT-4', as.numeric(task) - dodge_spacing/4, ifelse(
                model == 'GPT-3.5', as.numeric(task) - dodge_spacing/8, ifelse(
                    model == 'LLaMA2-70B', as.numeric(task), NA))),
        xstart = as.numeric(task) - 3*dodge_spacing/8,
        xend = ifelse(psig == '', xstart, ifelse(
            model == 'GPT-4', as.numeric(task) - dodge_spacing/8, ifelse(
                model == 'GPT-3.5', as.numeric(task) + dodge_spacing/8, ifelse(
                    model == 'LLaMA2-70B', as.numeric(task) + 3*dodge_spacing/8, NA)))))  %>%
    ggplot(aes(x = task, y = score, fill = model, colour = model)) + 
    geom_point(position = position_jitterdodge(jitter.width = 0.1, dodge.width = dodge_spacing)) + 
    geom_violin(alpha = .5, position = position_dodge(dodge_spacing), scale = 3) + 
    stat_summary(aes(group = model), geom = 'point', fun.y = 'median', position = position_dodge(dodge_spacing), colour = 'black') +
    xlab('Task') + ylab('Response score (fraction correct)') +
    geom_text(aes(y = ymax + 0.01, x = xdodge, label = psig), colour = 'black') + 
    geom_segment(aes(x = xstart, xend = xend, y = ymax, yend = ymax)) + 
    scale_y_continuous(limits = c(0,1 + 4*sig_spacing), breaks = c(0,0.25,0.50,0.75,1), labels = c(0,0.25,0.50,0.75,1))

f1A_violin <- f1A_violin + th + scale_colour_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA)) + 
  scale_fill_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA))
f1A_violin$layers[[1]]$aes_params$size <- ps

print(f1A_violin)
```

#### Analysis - Original vs. New items
Ceiling performance on tests could indicate that these items are present in the original dataset on which these LLMs were trained (except for Irony, which was not publicly available at the time of testing). In order to control for this, we ran a series of corrected Wilcoxon Tests comparing the models' performance on each test on original items vs. novel items to see if they performed significantly worse. Once again, we excluded the False Belief condition as the performance was perfect for both models

```{r}
comps_new <- df1 %>% filter(task != 'Irony' & task != 'False Belief') %>%
    group_by(task,model) %>% 
    wilcox_test(score ~ source, detailed = T) %>% 
  adjust_pvalue(method = mccor) %>% 
  mutate(p.adj.signif = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '\\*',''))))

comps_new %>% 
  select(-.y., -method, -alternative, -p, -n1, -n2)%>% kable() %>% kable_styling() 
```
#### Figure 1B - Barbell plot
Performance of human (purple) and two LLMs (GPT-3.5, light blue; GPT-4, green) benchmark responses on a battery of Theory of Mind tests. Barbell plot showing 68% binomial confidence intervals scores of each model on the original test items (dark colours) and new test items (pale colours) across each test. Tests are ordered in descending order of human performance (top plot). Significance markers indicate the results of corrected Wilcoxon two-way tests comparing LLM performance against human baseline (* p<.05; ** p<.01; *** p<.001). 

```{r, fig.width = 9, fig.height = 5, out.width='100%'}
f1B_barbell <-  df1 %>%
    rowwise() %>%
    ungroup() %>%
    group_by(task,model,source) %>%
    summarise(low = quantile(score)[2],
              high = quantile(score)[4],
              median = median(score, na.rm = T)) %>%
    ungroup() %>%
    mutate(
        psig = factor(apply(., 1, function(x) ifelse(x['task'] %in% c('False Belief', 'Irony'), '', eval(comps_new$p.adj.signif[comps_new$model == x['model'] & comps_new$task == x['task']]))),
                      levels = c('***','**','\\*',''),
                      labels = c('***','**','*','')),
        ymax = apply(., 1, function(x) max(high[model == x['model'] & task == x['task']]) + 0.05),
        col = factor(sprintf('%s_%s',model,source),
                     levels = c('Human_New','Human_Original',
                                'GPT-4_New','GPT-4_Original',
                                'GPT-3.5_New','GPT-3.5_Original',
                                'LLaMA2-70B_New','LLaMA2-70B_Original'
                     )),
        source = factor(source, levels = c('Original','New'))) %>%
    mutate(task = factor(ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)), levels = order$task_narrow),
           high = ifelse(task == 'Irony', NA, high),
           low = ifelse(task == 'Irony', NA, low),
           median = ifelse(task == 'Irony', NA, median)) %>%
    ggplot(aes(x = model, y = median, group = source)) +
    geom_errorbar(aes(ymin = low, ymax = high), width = .1, position = position_dodge(0.5)) +
    geom_point(aes(y = low, colour = col), size = ps_l, position = position_dodge(0.5)) +
    geom_point(aes(y = high, colour = col), size = ps_l, position = position_dodge(0.5)) +
    geom_point(aes(y = median, colour = col), size = ps_l, shape = 5, position = position_dodge(0.5)) +
    scale_colour_manual(values = c(createColourValues(cp$Human)[3],cp$Human,
                                   createColourValues(cp$GPT4)[3],cp$GPT4,
                                   createColourValues(cp$ChatGPT)[3],cp$ChatGPT,
                                   createColourValues(cp$LLaMA)[3],cp$LLaMA)) +
    xlab('') + ylab('Response score (fraction correct)') +
    scale_y_continuous(limits = c(0,1.05), breaks = c(0, 0.25, 0.5, 0.75, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) +
    facet_grid(.~task, switch = 'both') +
    th + theme(axis.line.y = element_blank(), axis.text.x = element_blank(), axis.line.x = element_blank(), axis.ticks.x = element_blank(), panel.border = element_rect(colour = 'grey', fill = NA)) +
    geom_text(aes(y = ymax, label = psig))
  
print(f1B_barbell)

```

#### Ironic vs. Non-ironic items

The Irony comprehension test consisted of both ironic and non-ironic utterances. Non-ironic utterances are intended to serve as a control to rule out explanations of random guessing or response bias. Here, we break down the responses by trial type and model to see where errors occur

```{r}
rbind(df %>%
        mutate(                          
              task = apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])),
              source = factor(source, levels = c('old','new'), labels = c('Original', 'New'))) %>%
        filter(task == 'Irony') %>% 
        gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
        mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15))),
               trial_state = factor(trial_state, levels = c('I','N'), labels = c('Ironic','Non-ironic'))) %>%
          filter(!is.na(score)) %>%
        group_by(model, trial_state) %>% 
        summarise(total = n(), sum = sum(score), score = mean(score)),
  dfh %>%
        mutate(                          
            task = apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])),
            source = factor(source, levels = c('old','new'), labels = c('Original', 'New'))) %>%
      filter(task == 'Irony') %>% 
      gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
      mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15))),
             trial_state = factor(trial_state, levels = c('I','N'), labels = c('Ironic','Non-ironic'))) %>%
        filter(!is.na(score)) %>%
      group_by(model, trial_state) %>% 
      summarise(total = n(), sum = sum(score), score = mean(score))) %>% 
  kable() %>% kable_styling()

```

Humans and GPT-4 appear to be equally good at recognising both kinds of utterance. GPT-3.5 performs well as interpreting literal utterances but fails specifically with ironic ones. LLaMA2-70B appears to struggle with interpreting both kinds of answers, which could indicate chance performance. 

### Understanding faux pas

```{r}
# load data from follow-up 
df_fp_fu <- read.csv('scored_data/scores_followup.csv', header = T)

# arrange data into a dataframe. The qualitative outcome types are not relevant for the main analysis but are described in the Supplementary Material Section S3
df_fp2 <- df_fp_fu %>% 
  gather(trial, type, -task, -item, -source, -trial_state, -model) %>% 
  mutate(outcome = factor(ifelse(type %in% c('A*','A+','A','A-','D'), 'Success',
                        ifelse(type %in% c('B','C'), 'Mixed success',
                               ifelse(type %in% c('E','F'),'Failure',NA))),
                        levels = c('Success','Mixed success','Failure',NA)),
         score = (3-as.numeric(outcome))/2) %>%
  filter(!is.na(score))

# extract the faux pas items from the original dataset
df_followup <- df %>% 
  filter(task == 'Faux Pas' & model != 'LLaMA-70B') %>% 
  gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
  mutate(question = '2afc')

# collate the original and follow-up data into a single dataframe 
df_followup <- rbind(df_followup,
             df_fp2 %>% select(-type,-outcome) %>% mutate(question = 'likely')) 
```

#### Figure 2A: Raincloud comparing original and follow-up 
Side-by-side comparison of performance on the Faux Pas test using the original question ('Did they know…?', left) and the updated likelihood question (right). Individual data points and lines show data aggregated by test item across sessions.

```{r, out.width='100%'}
f2A_FP2 <- df_followup %>% 
    group_by(item,model,question) %>% summarise(score = mean(score)) %>% mutate(point_id = sprintf('%s_%s',item, model)) %>% 
    ggplot(aes(x = factor(question, levels = c('2afc','likely'), labels = c("Did they know...?","Is it more likely\nthat they knew\nor didn't know...?")), 
               y = score, 
               fill = model)) +
    stat_summary(aes(group = point_id, colour = model), geom = 'point', fun = 'mean') +
    stat_summary(aes(group = point_id, colour = model), geom = 'line', fun = 'mean') +
    stat_halfeye(data = . %>% dplyr::filter(question == '2afc'), position = position_nudge(-0.15), side = 'left', alpha = .5) + 
    stat_halfeye(data = . %>% dplyr::filter(question == 'likely'), position = position_nudge(0.15), side = 'right', alpha = .5) + 
    th + scale_fill_manual(values = c(cp$ChatGPT,cp$GPT4)) + scale_colour_manual(values = c(cp$ChatGPT,cp$GPT4)) + 
    xlab('') + ylab('Response score (fraction correct)')
  
print(f2A_FP2)
```

### Testing information integration

```{r, warning=F}
df_variants <- read.csv('scored_data/scores_variants.csv', header = T)
```

```{r, warning=FALSE}
dfv_fp <- df_variants %>% filter(Task == 'Faux Pas')  %>% 
    mutate_at(vars(-Task,-Story,-Type), ~as.numeric(.))
```

Framing the belief question of the Faux Pas test in terms of likelihood led to better performance in GPT models such that they performed similarly to humans and LLaMA2-70B. This formulation of the question raises the concern that responses could be driven by a response bias. 

In order to explore this, we ran three variants of six stories that involved utterances that were either a faux pas that implied the person didn't know, a statement that implied the person did know, or statements that were unrelated and so conveyed no implication of knowledge one way or another. We showed these to participants (separate participants for each item, separate LLM chats for each item) and asked them whether it was more likely the speaker knew or didn't know. 

"Didn't know" responses were coded -1, "Knew" responses were coded +1, and responses that said it was impossible to say one way or another were coded as 0. Ambiguous responses (e.g. "More/less likely", "Yes/No"; 2.51% of data) were treated as if the question were worded "Is it more likely the person knew". Non-answers that did not answer the question at all (e.g. "It didn't matter", "She didn't care") were coded as NAs. 7 responses were further removed on the suspicion that these responses were generated by LLMs following unanimous agreement of all coders. Coded responses were averaged across story to compute directional scores such that negative values indicate that the model was more likely to respond that the person didn't know and positive scores indicate they were more likely to respond that the person did know. 

#### Analysis - Chi square tests comparing Faux Pas and Knowledge Implied variants against Neutral

```{r}
comp_fpv <- dfv_fp %>% 
    mutate(Type = factor(Type,
                         levels = c('Faux Pas','Neutral','Knowledge-Implied'))) %>%
    select(-Task) %>% 
    gather(trial,score,-Type,-Story) %>% 
    mutate(
        model = factor(apply(., 1, function(x) strsplit(x['trial'],'\\.')[[1]][1]),
                       levels = c('score_hum','score_v4','score_v3','score_70B'),
                       labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B'))
    ) %>% 
    filter(!is.na(score)) %>% 
    group_by(model,Type,score) %>% 
    summarise(n = n()) %>% 
    mutate(score = factor(score, levels = c(-1,0,1), labels = c('DidntKnow','Unsure','Knew'))) %>% 
    ungroup() %>% 
    nest_by(model) %>% 
    mutate(matrix1 = list(map(data$score, ~ data %>% ungroup %>% 
                                  filter(Type != 'Knowledge-Implied') %>% 
                                  spread(n, key = score) %>% 
                                  tibble::column_to_rownames(., 'Type') %>% 
                                  as.matrix(.))),
           x_1 = list(map(matrix1, ~ chisq.test(.)$statistic)),
           df_1 = list(map(matrix1, ~ chisq.test(.)$parameter)),
           p_1 = list(map(matrix1, ~ chisq.test(.)$p.value)),
           matrix2 = list(map(data$score, ~ data %>% ungroup %>% 
                                  filter(Type != 'Faux Pas') %>% 
                                  spread(n, key = score) %>% 
                                  tibble::column_to_rownames(., 'Type') %>% 
                                  as.matrix(.))),
           x_2 = list(map(matrix2, ~ chisq.test(.)$statistic)),
           df_2 = list(map(matrix2, ~ chisq.test(.)$parameter)),
           p_2 = list(map(matrix2, ~ chisq.test(.)$p.value))) %>% 
    unnest(c(x_1,df_1,p_1,x_2,df_2,p_2)) %>% 
    select(-data, -matrix1, -matrix2) %>% as.data.frame() %>% unique() %>% 
    pivot_longer(., -model, names_sep = '_', names_to = c('stat','contrast')) %>% 
    mutate(value = unlist(value)) %>% 
    pivot_wider(., names_from = 'stat', values_from = 'value') %>% 
    mutate(contrast = factor(contrast, levels = c(1,2), labels = c('Faux Pas','Knowledge-Implied'))) %>% 
    adjust_pvalue(method = mccor) %>% # adjust for multiple comparisons 
    mutate(p.adj.signif = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '\\*',''))))
  
comp_fpv %>% kable() %>% kable_styling()
```

#### Figure 2B. Bar plot showing bias of responses on variants 

Bars show the averaged response codes to the final question of the three conditions, with negative (left) values indicating a bias towards saying the person did not know the context, and positive (right) values indicating a bias towards saying they knew. Error bars show 68% binomial confidence intervals. Note that due to the multinomial nature of these data (with 0 as an 'unsure' response), binomial confidence intervals are slightly inaccurate, but these unsure responses were generally rare and only occurred for human and GPT-4.

```{r, out.width='100%'}
f <- dfv_fp %>% 
    mutate(Type = factor(Type,
                         levels = c('Faux Pas','Neutral','Knowledge-Implied'))) %>%
    select(-Task) %>% 
    gather(trial,score,-Type,-Story) %>% 
    mutate(
        model = factor(apply(., 1, function(x) strsplit(x['trial'],'\\.')[[1]][1]),
                       levels = c('score_hum','score_v4','score_v3','score_70B'),
                       labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B')),
        score_fact = factor(score,
                            levels = c(-1,0,1,NA),
                            labels = c("Didn't know","Unsure","Knew"))
    ) %>% 
    mutate(model = factor(model, levels = rev(levels(model)))) %>% 
    arrange(model, Type) %>% 
    filter(!is.na(score)) %>% 
    group_by(model,Type) %>% 
    summarise(mean = mean_ci(score, ci = 0.95),
              ymin = ifelse(mean$y < 0, mean$ymin - 0.05, mean$ymax + 0.05),
              ymax = ifelse(mean$y < 0, mean$ymin - 0.08, mean$ymax + 0.08)) %>% 
    ungroup() %>% 
    mutate(xdodge = as.numeric(model) + (-0.3 + as.numeric(Type) * 0.15),
           nmin = apply(., 1, function(x) .$ymin[.$Type == 'Neutral' & .$model == x['model']])) %>% 
    filter(Type != 'Neutral') %>% 
    mutate(p.signif = apply(., 1, function(x) 
        comp_fpv$p.adj.signif[comp_fpv$model == x['model'] & comp_fpv$contrast == x['Type']])) %>% 
    mutate(
        xstart = ifelse(Type == 'Faux Pas', as.numeric(model) - 0.3, as.numeric(model)),
        xend = ifelse(p.signif == '', xstart, ifelse(
            Type == 'Faux Pas', as.numeric(model), as.numeric(model) + 0.3)),
        ymin = ifelse(p.signif == '', ymax, ymin),
        ytext = ifelse(ymax > 0, ymax + 0.075, ymax - 0.075)
    )

f2B_fpv <- dfv_fp %>% 
    mutate(Type = factor(Type,
                         levels = c('Faux Pas','Neutral','Knowledge-Implied'))) %>%
    select(-Task) %>% 
    gather(trial,score,-Type,-Story) %>% 
    mutate(
        model = factor(apply(., 1, function(x) strsplit(x['trial'],'\\.')[[1]][1]),
                       levels = c('score_hum','score_v4','score_v3','score_70B'),
                       labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B')),
        score_fact = factor(score,
                            levels = c(-1,0,1,NA),
                            labels = c("Didn't know","Unsure","Knew"))
    ) %>% 
    filter(!is.na(score)) %>% 
    ggplot(aes(x = model, y = score)) + 
    geom_hline(yintercept = 0, colour = 'grey')+
    stat_summary(aes(group = Type, fill = Type), geom = 'bar', position = position_dodge(0.9), fun.data = 'mean_ci') +
    stat_summary(aes(group = Type), geom = 'pointrange', size = .1, position = position_dodge(0.9), 
                 fun.data = function(x) { 
                     BinomCI(x = sum((x+1)/2), n = length(x), conf.level = .68, method = 'clopper-pearson') %>% 
                         as.data.frame() %>% 
                         mutate_all(., ~.*2-1) %>% 
                         rename(y = 'est', ymin = 'lwr.ci', ymax = 'upr.ci') }, 
                 colour = 'black') +
    th + scale_fill_manual(values = c(cp$fauxpas,cp$neutral,cp$knowImp)) + 
    xlab('') + ylab('"More likely that they..."') +
    scale_y_continuous(#limits = c(-1.2,1.2), 
        breaks = c(-1,0,1), labels = c("Didn't know\n(-1)","Unsure\n(0)","Knew\n(+1)")) + 
    geom_text(data = f, aes(x = xdodge, y = ytext, label = p.signif)) + 
    geom_segment(data = f, aes(x = xstart, xend = xend, y = ymax, yend = ymax)) + 
    geom_segment(data = f, aes(x = xstart, xend = xstart, y = ymax, yend = ymin)) + 
    geom_segment(data = f, aes(x = xend, xend = xend, y = ymax, yend = ymin)) + 
    coord_flip() +
    scale_x_discrete(limits = rev(levels(df1$model)))

print(f2B_fpv)
```

## Supplementary Material {.unnumbered .active}
<h2>Supplementary Material</h2>
This is the supplementary material for Strachan et al. (2023) _Testing Theory of Mind in GPT Models and Humans_. Below, you will find the code, output, and figures of supplementary analyses that were not included in the final manuscript. 

### 1. Comparison of LLaMA2-Chat Models

```{r}
df_llama <- read.csv('scored_data/scores_llama.csv', header = T)

dfl <- df_llama %>%
    filter(! trial_state %in% c('FC','TC')) %>%     # get old items
    mutate(                          
        task = apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])),
        source = factor(source, levels = c('old','new'), labels = c('Original', 'New'))) %>% 
    gather(col, score, -task, -item, -source, -trial_state) %>% 
    mutate(trial = apply(., 1, function(x) strsplit(x['col'], '\\.')[[1]][2]),
           model = factor(apply(., 1, function(x) strsplit(x['col'], '\\.')[[1]][1]),
                          levels = c('score_70B','score_13B','score_7B'),
                          labels = c('LLaMA2-70B','LLaMA2-13B','LLaMA2-7B'))) %>%
    filter(!is.na(score)) %>%
  group_by(task, trial, source, model) %>%
  summarise(score = mean(score, na.rm = T))

# by-task model comparison 
comps_llama <- dfl %>% 
  mutate(task = factor(task, levels = order$task)) %>% 
    group_by(task, trial, model) %>%
    summarise(score = mean(score, na.rm = T)) %>% 
    ungroup() %>% 
  rowwise() %>%
  mutate(task = factor(ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)))) %>%
  ungroup() %>%
  group_by(task) %>%
  wilcox_test(score ~ model, detailed = T) %>% 
  adjust_pvalue(method = mccor) %>% 
  mutate(p.adj.signif = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '*','')))) %>% 
  select(-.y., -method, -alternative, -p, -n1, -n2)

comps_llama70 <- comps_llama %>% filter(group1 == 'LLaMA2-70B')

```

We collected data on the full Theory of Mind battery for three LLaMA2-Chat models, sized at 7 billion (7B), 13 billion (13B), and 70 billion (70B) parameters. Performance of the three LLaMA2-Chat models is shown in Figure S1. Numerical values for statistical comparisons are reported in Table S1. 

```{r FigS1, out.width='70%', fig.cap="Performance of three sizes of LLaMA2-Chat models"}
fs1_violin <- dfl %>% 
  rowwise() %>%
  mutate(task = factor(ifelse(grepl(' ', task), gsub(' ','\n', task), as.character(task)), levels = order$task_narrow)) %>%
  ungroup() %>%
  mutate(
      psig = factor(apply(., 1, function(x) comps_llama70$p.adj.signif[comps_llama70$group2 == x['model'] & comps_llama70$task == x['task']]),
                      levels = c('***','**','*',''),
                      labels = c('***','**','*','')),
      ymax = ifelse(
          model == 'LLaMA2-13B', 1 + sig_spacing, ifelse(
              model == 'LLaMA2-7B', 1 + 2*sig_spacing, NA)),
      xdodge = ifelse(
          model == 'LLaMA2-13B', as.numeric(task) - dodge_spacing/6, ifelse(
              model == 'LLaMA2-7B', as.numeric(task), NA)),
      xstart = as.numeric(task) - dodge_spacing/3,
      xend = ifelse(psig == '', xstart, ifelse(
          model == 'LLaMA2-13B', as.numeric(task), ifelse(
              model == 'LLaMA2-7B', as.numeric(task) + dodge_spacing/3, NA)))) %>% 
  mutate(task = factor(task, levels = order$task_narrow)) %>%
  ggplot(aes(x = task, y = score, fill = model, colour = model)) + 
  geom_point(position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.7)) + 
  geom_violin(alpha = .5, position = position_dodge(0.7), scale = 3) + 
  stat_summary(aes(group = model), geom = 'point', fun.data = 'mean_cl_boot', position = position_dodge(0.7), colour = 'black') +
  xlab('Task') + ylab('Response score (fraction correct)') +
  th + theme(legend.position = 'top') +
  scale_fill_manual(values = createColourValues(cp$LLaMA, n = 3)) + 
  scale_colour_manual(values = createColourValues(cp$LLaMA, n = 3))  +
  geom_text(aes(y = ymax + 0.01, x = xdodge, label = psig), colour = 'black') + 
  geom_segment(aes(x = xstart, xend = xend, y = ymax, yend = ymax)) 

fs1_violin$layers[[1]]$aes_params$size <- ps

print(fs1_violin)
ggsave(fs1_violin, filename = 'Figures/FigureS1.png', width = 8, height = 6, dpi = 300)
```
Unlike the other models, both LLaMA2-7B and LLaMA2-13B provided a large proportion of non-answer responses (e.g. “Sure! Go ahead and tell me the story” despite the story and questions already being posed, or answers that cut off halfway through answering the question). In total, approximately 22% of LLaMA2-7B responses and 10% of LLaMA2-13B responses were non-answers. To obtain a complete dataset, whenever we encountered a non-answer, we regenerated a response until we obtained a codable response. LLaMA2-70B significantly outperform the smaller models in all tests except in the Irony comprehension test.

```{r TabS1}
comps_llama %>% flextable() %>% 
  set_caption("Pairwise comparisons (corrected Wilcoxon tests) of three LLaMA2-Chat models across tests in the Theory of Mind Battery") %>%
  colformat_double() %>% 
  set_header_labels(
    task = 'Task',
    estimate = 'Est.',
    group1 = 'Model 1',
    group2 = 'Model 2',
    statistic = 'Statistic',conf.low = 'CI-',conf.high = 'CI+',p.adj = 'p (corr.)',p.adj.signif = 'p<.05') %>% 
  align(align = c('left','center','left','left','center','center','center','center','center')) %>% 
  align(align = 'center', part = 'header')
```

\newpage
### 2. Variability of performance across test items

```{r,include=F}
dfi <- df %>% 
  gather(trial, score, -task,-item,-source,-trial_state,-model) %>% 
  mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>% 
  mutate(trial_state = factor(trial_state, levels = c('-','F','T','I','N'), labels = c('','False','True','Ironic','Non-ironic'))) %>% 
  filter(!is.na(score))

dfi <- rbind(dfi, dfh %>% 
               gather(trial, score, -task,-item,-source,-trial_state,-model) %>% 
               mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>% 
               mutate(trial_state = factor(trial_state, levels = c('-','F','T','I','N'), labels = c('','False','True','Ironic','Non-ironic'))) %>% 
               filter(!is.na(score)))

dfi[dfi$task %in% c('False Belief A','False Belief B') & dfi$item == 8,'item'] <- 7
```

```{r FigS2, fig.width = 8, fig.height = 10, out.width='80%', fig.cap='Means and 68% binomial confidence intervals across trials of the four experimental models on each item within the test. We report 68% Cis because they correspond to 1 standard deviation for Gaussian distributions. For each model and each task, the coefficient of variation (CV) is shown above the plot. A and B for the False Belief and Irony tasks are separated out in this figure into the two set lists used in the study on which the order or items remained the same but the trial state (False/True Belief; Ironic/Non-ironic) varied from trial to trial. For these two tests, trial state is denoted by point shape'}
fs2 <- dfi %>% 
    group_by(task, item, trial_state, model) %>% 
    mutate(model = factor(model, levels = c('Human','GPT-4','GPT-3.5','LLaMA-70B'),
                          labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B')),
           task = factor(task,
                         levels = c('False Belief A','False Belief B','Irony A','Irony B','Faux Pas','Hinting','Strange Stories'),
                         labels = c('False Belief A','False Belief B','Irony A','Irony B','Faux Pas','Hinting','Strange Stories')),
           trial_state = as.character(trial_state)) %>% 
    summarise(x = sum(score),
              n = n()) %>%
    mutate(mean  = BinomCI(x,n, conf.level = 0.68, sides = 'two.sided', method = 'clopper-pearson')[,'est'],
           lower = BinomCI(x,n, conf.level = 0.68, sides = 'two.sided', method = 'clopper-pearson')[,'lwr.ci'],
           upper = BinomCI(x,n, conf.level = 0.68, sides = 'two.sided', method = 'clopper-pearson')[,'upr.ci']) %>% 
    ungroup() %>%
    group_by(model, task) %>% 
    mutate(cv = sd(mean) / mean(mean)) %>% 
    ggplot(aes(x = item, y = mean, fill = model)) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .5) +
    geom_line() + 
    geom_point(size = ps) + 
    geom_point(aes(shape = trial_state), size = ps_l) + 
    geom_text(aes(label = sprintf('CV=%.02f',cv)), x = 3.2, y = 1.15) +
    facet_grid(task ~ model, scales = 'free_x') +
    scale_fill_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA)) + 
    scale_y_continuous(limits = c(0, 1.2), breaks = c(0,0.5,1)) + 
    xlab('Item in test') + ylab('Response score (fraction correct)') + 
  guides(shape = guide_legend(nrow = 2), fill = guide_legend(nrow = 2)) +
  scale_shape_manual(values = c(15,17,15,17,20), 
                     breaks = c('False','True','Ironic','Non-ironic'), 
                     labels = c("False","True","Ironic","Non-ironic")) + th 

print(fs2)
ggsave(fs2, filename = 'Figures/FigureS2.png', width = 8, height = 10, dpi = 300)
```

Human responses to individual items on tests can be variable, as different people bring different intuitions or priors that affect their interpretation of particular stories. Figure S2 shows a breakdown of individual item performance for all models across all tests included in our Theory of Mind Battery. Comparing LLMs and human item-wise performance revealed no systematic patterns where humans and LLMs systematically failed on the same items within a test. 

To quantify the relative variability of human and model response scores across items, we computed, for each test and experimental model, the Coefficient of Variation (CV), that is, the ratio between the standard deviation of the mean response scores across items and the grand mean across items of the response scores. This analysis shows that while human responses were variable on some tests, there was low relative variability across test items within each test. For GPT-4, the CV in item-wise performance was also low on all tests except for the Faux Pas. GPT-3.5 and LLaMA2-70B showed higher CVs. Specifically, GPT-3.5 showed higher CV in item-wise performance on Irony, Faux Pas, and Strange Stories. LLaMA2-70B showed higher CV on Irony, Hinting, and Strange Stories. 

### 3. Effects of item position 

In the Theory of Mind Battery, each chat with LLMs was a separate and independent session, ruling out between-session order effects. However, since all models remember previous messages within an individual chat session, this introduces the potential for order effects driven by the position of an item within the session. 

To test for order effects at the item level, we fit a binary logistic regression (quasibinomial for Strange Stories) to individual item scores on the original test items using item position as a predictor for each model on each test. Due to perfect performance in the False Belief test, this test was not included in this analysis. Results are shown in Table 2. 

GPT-4 and LLaMA2-70B did not show any effects of item position across any test. GPT-3.5 showed significant item order effects on response scores for the Faux Pas, Strange Stories, and the Irony tests, but not for Hinting. For the Faux Pas test, the slop of the effect was negative such that GPT-3.5 performed worse on later items than on earlier ones, while for the Strange Stories and Irony tests, the slope was positive indicating that the model performed better on later than on earlier items. 

```{r}
df_learning <- df %>%
  filter(source == 'old', !task %in% c('False Belief A','False Belief B')) %>%     # get original items
  mutate(                          
    task = factor(apply(., 1, function(x) ifelse(grepl(' A', x['task']) | grepl(' B', x['task']), substr(x['task'], 1, nchar(x['task'])-2), x['task'])), levels = c(order$task[order$task != 'False Belief'])),
    model = factor(model, levels = c('GPT-4','GPT-3.5','LLaMA-70B'), labels = c('GPT-4','GPT-3.5','LLaMA2-70B'))
  ) %>% 
  gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
  mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>% 
  mutate(trial_state = factor(trial_state, levels = c('-','F','T','I','N'), labels = c('','False','True','Ironic','Non-ironic'))) %>% 
  filter(!is.na(score))
```

```{r TabS2}
m <- df_learning %>% 
    nest_by(task, model) %>%
    mutate(fit = list(
        map(., 
            ~ifelse(task == 'Strange Stories', 
                list(tidy(glm(score ~ item, data = data, family = quasibinomial(link = "logit")))),
                list(tidy(glm(score ~ item, data = data, family = binomial(link = "logit")))))))) %>% 
    mutate(estimate = fit$model[[1]]$estimate[2],
           std.error = fit$model[[1]]$std.error[2],
           statistic = fit$model[[1]]$statistic[2],
           p = fit$model[[1]]$p.value[2]) %>%
    select(-data,-fit) %>% 
    adjust_pvalue(method = mccor, p.col = 'p') %>% 
    mutate(sig = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '*',''))))

m %>% flextable() %>% set_caption("Effect of item position for each test")  %>% colformat_double() %>% 
  set_header_labels(task = 'Task',model = 'Model', estimate = 'Est',std.error = 'SE',
                    statistic = 'Statistic',p.adj = 'p (corr.)', sig = 'p<.05') %>% 
  align(align = c('left','left',rep('center',6))) %>% 
  align(align = 'center', part = 'header')
```

These effects could indicate that item ordering influenced GPT-3.5’s performance. However, because in the original testing protocols items were presented in the fixed order prescribed by the original validated version of each test (see Methods), they could also reflect difficulties related to specific items and their distribution within a given session. To isolate order effects from other item-specific effects, we collected another set of data with GPT-3.5 presenting items in a randomised order for each session on the Faux Pas, the Strange Stories, and the Irony Comprehension tests. To determine how many follow-up samples we need to collect, we conducted a power analysis using the learning effects identified with GPT-3.5. The most conservative effect size to use for estimating required sample size was for the Irony test. As such, we fit a power curve to estimate the number of necessary trials using the powerSim package in R that runs a number of simulations (n=1000) over a range of sample sizes to estimate statistical power. This analysis indicated that 12 sessions would be sufficient to provide 80% power. The testing was identical to the protocol used for the Theory of Mind Battery with the exception that all items were presented in a randomised order and that for the Irony test only ironic items were included. 

Fitting a (quasi-)binomial logistic regression to predict scores as a function of trial position revealed an order effect for the Irony test, whereby GPT-3.5 made more errors on earlier trials than later ones. In contrast, errors in Faux Pas and Strange Stories did not exhibit an order effect. 

The results of this randomised order dataset are shown in Figure S3 and Table S3. 

```{r}
dfsh <- read.csv(file = 'scored_data/scores_shuffled.csv', header = T)

# rescale Strange Stories to be within 0 and 1 
dfsh <- dfsh %>% 
  mutate(score = ifelse(task == 'Strange Stories', score/2, score))
```


```{r FigS3, out.width='70%', fig.width = 5, fig.height = 3, fig.cap="Item position curves on randomised orders"}
fs4_shuffle <- dfsh %>% mutate(task = factor(task, levels = c('Faux Pas','Strange Stories','Irony'))) %>% ggplot(aes(x = factor(order), y = score, group = task, colour = task)) +
    geom_jitter(height = 0.05, width = 0) + 
    geom_smooth(formula = y~x, method = 'glm', method.args = list(family = 'quasibinomial'), show.legend = F) +
    xlab('Trial position') + ylab('Response score') + facet_grid(.~task, scales = 'free_x') + 
    scale_colour_manual(values = c(rep(cp$ChatGPT,3))) + th + theme(legend.position = 'none')

print(fs4_shuffle)
ggsave(fs4_shuffle, filename = 'Figures/FigureS4.png', width = 5, height = 3, dpi = 300)
```
```{r TabS3}
# analyse the shuffled data
m2 <- dfsh %>% 
    nest_by(task) %>%
    mutate(fit = list(
        map(., 
            ~ifelse(task == 'Strange Stories', 
                list(tidy(glm(score ~ order, data = data, family = quasibinomial(link = "logit")))),
                list(tidy(glm(score ~ order, data = data, family = binomial(link = "logit")))))))) %>% 
    mutate(estimate = fit$data[[1]]$estimate[2],
           std.error = fit$data[[1]]$std.error[2],
           statistic = fit$data[[1]]$statistic[2],
           p = fit$data[[1]]$p.value[2]) %>%
    select(-data,-fit) %>% 
    adjust_pvalue(method = mccor, p.col = 'p') %>% 
    mutate(sig = ifelse(p.adj < .05, '*',''))

m2 %>% flextable() %>% set_caption("Effect of item position in shuffled data")  %>% colformat_double() %>% 
  set_header_labels(task = 'Task', estimate = 'Est', std.error = 'SE', statistic = 'Statistic',
                    p.adj = 'p (corr.)', sig = 'p<.05') %>% 
  align(align = c('left',rep('center',6))) %>% 
  align(align = 'center', part = 'header')
```

\newpage
### 4. False Belief Perturbations (adapted from Ullman 2023)

```{r, warning=F}
df_variants <- read.csv('scored_data/scores_variants.csv', header = T)
```

```{r}
dfv_fb <- df_variants %>% filter(Task == 'FB') %>% 
    mutate_at(vars(-Task,-Story,-Type), ~as.numeric(.))
```

In humans, success on the False Belief task requires inhibiting one’s own belief about reality in order to use one’s knowledge about the character’s mental state to derive predictions about their behaviour. However, with LLMs it is possible that performance on the False Belief task may be explained by lower level explanations than belief tracking. Supporting this interpretation, LLMs such as ChatGPT have been shown to be susceptible to minor alterations to the False Belief formulation (Ullman, 2023; Shapira et al., 2023) such as making the containers where the object is hidden transparent, or asking about the belief of the character who moved the object rather than the one who was out of the room. Such perturbations of the False Belief structure are assumed not to matter to entities that have Theory of Mind (Ullman, 2023). However, such an assumption has not been tested in humans. In order to compare the effects of these perturbations in humans and LLMs, we collected a new dataset of responses using five perturbations of three different False Belief stories. We also included a standard False Belief variant to replicate the models’ ceiling performance. The perturbations, adapted from Ullman (2023), we used were as follows:

* False Belief. The standard formulation as presented in the test battery. Example: 
    - *In the kitchen there are Lucy, Mia, a carton of orange juice, a fridge, and a cupboard. Lucy puts the carton of orange juice in the cupboard. She then leaves the kitchen and goes to school. While Lucy is away, Mia takes the orange juice out of the cupboard and puts it in the fridge. Mia leaves the room and goes to work. Lucy comes back from school and enters the kitchen. She doesn’t know what happened in the kitchen when she was away. When Lucy comes back home, where will she look for the orange juice?*
 
* Transparent. The containers in which the object was hidden were made of transparent plastic or glass so that the actor would not have to open them to see what was inside them
    - *Rather than a cupboard and a fridge, the containers are a transparent plastic box and a glass-fronted cabinet such that the juice can be readily seen inside*

* Preposition. The preposition in the stories was changed such that the object was no longer obscured (e.g. “in the box” became “on the box”)
    - *Rather than putting the juice carton _in_ the cupboard, Lucy puts it _on_ the cupboard, and similarly with Mia moving it to _on_ the fridge. When Lucy enters the room the story describes her looking around, so she should be able to see the juice immediately*

* Testimony. The agent who moved the object (the Mover) told the target character that they were going to move the object.
    - *All details are as in the original, except that Mia calls/texts Lucy to tell her that she is going to move the juice and Lucy believes her*

* Mover. The question asked about the belief of the Mover rather than the character who was out of the room. 
    - *All details are as in the original except that the question asks where _Mia_ will look for the juice*

We adapted three False Belief stories to generate variants for each, resulting in 15 new stories (for the full text, see Appendix 1, below). In order to control for any cross-influence between variants, we elected to test each item separately in a different chat for each LLM (n=15 repetitions per item), and with a new sample of ~50 humans (total N = 757). The results of these variants are shown in Figure S4.

```{r FigS4, out.width='60%', fig.width = 5, fig.height = 4, fig.cap="Performance of LLMs and humans across perturbations of the False Belief task"}
fs5_fbvars <- dfv_fb %>% 
    mutate(Type = factor(Type, 
                         levels = rev(c('False Belief','Transparent','Preposition','Testimony','Mover')),
                         labels = rev(c('False Belief','Transparent','Preposition','Testimony','Mover')))) %>% 
    select(-Task) %>%
    gather(trial,score,-Type,-Story) %>% 
    filter(!is.na(score)) %>% 
    mutate(
        model = factor(apply(., 1, function(x) strsplit(x['trial'],'\\.')[[1]][1]),
                       levels = c('score_hum','score_v4','score_v3','score_70B'),
                       labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B')),
        Story = factor(Story, levels = c(1:3), labels = c("John's cat","Lisa's juice","Grace's biscuits"))
    ) %>% 
    ggplot(aes(x = Type, y = score, group = model, fill = model)) +
    stat_summary(geom = 'bar', fun.data = 'mean_se', stat = 'identity', width = .2, position = position_dodge(0.4)) + 
    stat_summary(geom = 'point', fun.data = 'mean_se', colour = 'black', size = 2, position = position_dodge(0.4)) +  
    th + 
  #facet_grid(.~Story) + 
  coord_flip() +
    scale_fill_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA)) + # edit when human data available
    scale_y_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1), labels = c(0, 0.25, 0.5, 0.75, 1)) + 
    xlab('Variant') + ylab('Response score')

print(fs5_fbvars)

ggsave(fs5_fbvars, filename = 'Figures/FigureS5.png', width = 5, height = 4, dpi = 300)
```
We replicated the poor performance of GPT models observed by previous studies (e.g. Ullman, 2023), with both GPT-3.5 and GPT-4 failing on Transparent, Preposition and Testimony perturbations. LLaMA2-70B performed similarly poorly, although it consistently passed the Mover variant. Contradicting the assumption that these perturbations do not affect entities that have a Theory of Mind, humans also failed on Transparent and Preposition perturbations. Similar to LLMs, when the story involved transparent containers or changes to prepositions, humans were also likely to report that a character would look for the object where they left it. 

It is worth noting that these control variants present diverse challenges that go beyond tracking mental states, and may involve understanding physical properties, relationships between objects, and spatial reasoning capabilities. They also differ in terms of the type of belief updating: the variants where humans performed ‘poorly’ according to the intuitions proposed by Ullman are those where the character’s belief can only be updated after they return to the room, while other variants where performance is more successful involve manipulations of belief states that exist prior to returning. These results highlight the need for rigorous investigation that includes human validation and systematic manipulation of factors that are relevant to Theory of Mind.

\newpage
### 5. Faux Pas: Coding strategies

The Faux Pas task consists of vignettes describing an interaction where one character (the speaker) says something they should not have said, not knowing or not realising that they should not have said it. To understand that a faux pas has occurred, one must recognise this lack of knowledge or realisation. The coding strategy reported in the main manuscript focusses on this element by coding responses on the basis of how participants (GPT or humans) respond to the fourth comprehension question: “Did [the speaker] know/realise/remember [the information that made their statement inappropriate]?” In order to be coded as correct, the response to this question had to commit to the correct answer (“No”). We focused on this question because this was the key question about mental states that determined the interpretation of the faux pas.

This methodological choice of coding strategy is important, and it does reflect a departure from the strategy described in Baron-Cohen et al. (1999), where participants must answer all four comprehension questions correctly in order to pass the test. Here, we report the results where the same responses were coded with this strategy. Furthermore, we also consider an alternative coding strategy. We adopted a strict strategy where responses to the final question that equivocated or expressed a uncertainty were not marked as correct. As an exploratory analysis, we recoded responses where the correct answer was mentioned as a plausible alternative but was not explicitly endorsed as correct rather than incorrect, to see if the poor performance of GPT was driven by our penalising uncertainty.

#### Four-question coding

The results of the four-question coding scheme were consistent with those reported in the main manuscript (Figure S5). 

```{r}
# compile the results of our chosen coding strategy, reported in the main manuscript
df_fpc <- df %>% filter(task == 'Faux Pas') %>% 
    select(-source,-trial_state) %>% 
    gather(session,score,-task,-item,-model) %>% 
    group_by(task, session,model) %>% 
    spread(item, value = score) %>% mutate(session = factor(session, levels = sprintf('score%s',c(1:15)), labels = c(1:15))) 
df_hfp <- dfh %>% filter(task == 'Faux Pas') %>% 
    select(-source,-trial_state) %>% 
    gather(session,score,-task,-item,-model) %>% 
    group_by(task, session,model) %>% 
    spread(item, value = score) %>% mutate(session = factor(session, levels = sprintf('score%s',c(1:51)), labels = c(1:51))) 
df_fpc <- data.frame(rbind(df_fpc, df_hfp)) %>% mutate(coding = 'KO') 

# load in the results of the original, stricter coding strategy described in Baron-Cohen et al. (1999)
df_recode <- read.csv('scored_data/scores_recoded_q4.csv', header = T) %>% mutate(coding = '4Q')
colnames(df_fpc) <- colnames(df_recode)

df_rc <- data.frame(rbind(df_fpc, df_recode)) %>% 
  mutate(coding = factor(coding, levels = c('KO','4Q')),
         model = factor(model, levels = c('Human','GPT-4','GPT-3.5','LLaMA-70B'),
                        labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B'))) %>%
  select(session,model,coding,item1:item15) %>% gather(item,score,-session,-model,-coding) 
```

```{r FigS5, out.width='60%', fig.width = 5, fig.height = 4, fig.cap="Four-question coding strategy. Side-by-side comparison of Human and LLM performance on the Faux Pas test using the knowledge-only coding criteria ('Did they know...?' question only) and the four-question coding criteria (all four questions coded as correct). Individual data points and lines show data aggregated by test item across sessions. Density plots are not shown for LLaMA2-70B as there was too little variability."}
fs6_original <- df_rc %>% 
    group_by(item, model, coding) %>% 
    summarise(score = mean(score, na.rm = T)) %>% 
    mutate(model = factor(model, levels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B')),
           tm = sprintf('%s_%s',item,model),
           coding = factor(coding, levels = c('KO','4Q'), labels = c('Knowledge-only','4-Question'))) %>% 
    ggplot(aes(x = coding, y = score, fill = model)) +
    stat_summary(aes(group = tm, colour = model), geom = 'point', fun = 'mean') +
    stat_summary(aes(group = tm, colour = model), geom = 'line', fun = 'mean', alpha = .2) + 
    stat_halfeye(data = . %>% dplyr::filter(coding == 'Knowledge-only' & model != 'LLaMA2-70B'), position = position_nudge(-0.2), side = 'left', alpha = .5) + 
    stat_halfeye(data = . %>% dplyr::filter(coding == '4-Question' & model != 'LLaMA2-70B'), position = position_nudge(0.2), side = 'right', alpha = .5) + 
    scale_colour_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA)) +
    scale_fill_manual(values = c(cp$Human,cp$GPT4,cp$ChatGPT,cp$LLaMA)) +
    xlab('Coding') + ylab('Response score (fraction correct)') + th + theme(legend.position = 'top')

print(fs6_original)

ggsave(fs6_original, filename = 'Figures/FigureS6.png', width = 5, height = 4, dpi = 300)
```

The performance of LLMs was largely unchanged under the four-question coding scheme. For humans, the scores were significantly lower under the four-question coding scheme than under the knowledge-only scheme. Upon examination of the responses, this was driven by responses to the first comprehension question: “In the story, did someone say something they should not have said?” The goal of this question is to ensure that participants recognise that the speaker’s utterance could cause hurt or offence to the victim, and as such responses were marked correct only if participants responded, “Yes”. However, a sizeable minority of human participants appeared to interpret this question as one of moral judgement, and used the speaker’s lack of knowledge as justification for why they were not “in the wrong” for saying what they did (e.g. “no he didn't say anything wrong because didn’t know”). Furthermore, despite answering no to the first question, human participants could frequently identify the offensive statement when prompted (“Nothing ‘wrong’, but if you’re asking the question, probably that he doesn’t like apple”) and reliably recognised that the speaker was not aware of the context. 

In order to verify whether this reduction of the human scores affected our conclusions, we compared human and GPT responses under the four-question coding scheme. As shown in Table S4, despite higher error rates under the four-question coding scheme than the knowledge-only, humans still performed significantly better at the task than both GPT models, and LLaMA2-70B continued to perform better than humans overall. 

```{r TabS4}
comps4q <- df_rc %>% filter(coding == '4Q') %>% 
    group_by(model,session) %>% 
    summarise(score = mean(score)) %>% 
    ungroup() %>%
    wilcox_test(score ~ model, detailed = T) %>% 
    filter(group1 == 'Human') %>% 
    adjust_pvalue(method = mccor) %>% 
    mutate(p.adj.signif = ifelse(p.adj < .001,'***',ifelse(p.adj < .01, '**', ifelse(p.adj < .05, '*','')))) %>% 
    select(-.y., -method, -alternative, -p, -n1, -n2)

comps4q %>% flextable() %>% set_caption("Comparison of LLMs against humans under four-question coding ")  %>% colformat_double() %>%
  set_header_labels(estimate = 'Est.', group1 = 'Reference', group2 = 'Model',
                    statistic = 'Statistic', conf.low = 'CI-', conf.high = 'CI+',
                    p.adj = 'p (corr.)', p.adj.signif = 'p<.05') %>% 
  align(align = c('center','left','left',rep('center',5))) %>% 
  align(align = 'center', part = 'header')
```

#### Alternative Coding Scheme

The uncertainty of GPT-3.5 and GPT-4 in answering the Faux Pas questions was frequently attributed to the answer not being present or directly mentioned within the story (“It is not clear from the story whether [they] knew”). Responses to some items indicated that GPT models could consider the correct answer as plausible but did not consider it more plausible than other alternatives (“it could be that [they] did not know, or that [they] knew and were just expressing an opinion”). The coding criteria for this task were strict such that responses to the two-alternative question, “Did [the Speaker] know…?” were only coded as correct if they committed to the answer ‘No’. It is possible that this strict 'Commit' coding approach penalized the performance of GPT models. In order to control for this, we recoded the responses of both GPT models and LLaMA2-70B the original Faux Pas task to mark as correct any responses that acknowledge consideration of the correct answer (‘No, the Speaker did not know/remember the context’), even if they did not commit to it (e.g. ‘The Speaker might not have remembered the context, or they might have remembered’ would have been marked incorrect under the first (Commit) coding scheme and correct under the new (Consider) one). As shown in Figure S6, this recoding resulted in marginal improvements in score that did not significantly affect the overall task performance.

```{r}
#  prepare the strict coding scheme from the main manuscript 
df_fp <- df %>% 
  filter(task == 'Faux Pas') %>%
  mutate(coding = 'Strict')

# load in the new, alternative coding scheme
df_recode <- read.csv('scored_data/scores_recoded.csv', header = T) %>% 
  mutate(coding = 'New')

df_rc <- data.frame(rbind(df_fp, df_recode)) %>% 
  mutate(coding = factor(coding, levels = c('Strict','New')))

df_rc <- df_rc %>% 
  select(item, model, coding, score1:score15) %>% 
  gather(trial, score, -item, -model, -coding) 
```

```{r FigS6, out.width='60%', fig.width = 5, fig.height = 4, fig.cap="Alternative coding strategy. Side-by-side comparison of GPT performance on the Faux Pas test using the strict coding criteria (‘Did they know…?’ answer only accepted if ‘No’ actively endorsed) and the new alternative coding (coded as correct if ‘No’ was considered a viable option but not actively endorsed). Individual data points and lines show data aggregated by test item across sessions. Density plots are not shown for LLaMA2-70B as there was too little variability."}
fs7_recode <- df_rc %>% 
    group_by(trial, model, coding) %>% 
    summarise(score = mean(score)) %>% 
    mutate(
      model = factor(model, 
                     levels = c('GPT-4','GPT-3.5','LLaMA-70B'), 
                     labels = c('GPT-4','GPT-3.5','LLaMA2-70B')),
      tm = sprintf('%s_%s',trial,model),
      coding = factor(coding, levels = c('Strict','New'), labels = c('Commit','Consider'))) %>% 
    ggplot(aes(x = coding, y = score, fill = model)) +
    stat_summary(aes(group = tm, colour = model), geom = 'point', fun = 'mean') +
    stat_summary(aes(group = tm, colour = model), geom = 'line', fun = 'mean') + 
    stat_halfeye(data = . %>% dplyr::filter(coding == 'Commit' & model != 'LLaMA2-70B'), position = position_nudge(-0.2), side = 'left', alpha = .5) + 
    stat_halfeye(data = . %>% dplyr::filter(coding == 'Consider'), position = position_nudge(0.2), side = 'right', alpha = .5) + 
    sf + sc +
    xlab('Coding') + ylab('Response score (fraction correct)') + th + theme(legend.position = 'top')

print(fs7_recode)
ggsave(fs7_recode, filename = 'Figures/FigureS7.png', width = 5, height = 4, dpi = 300)
```

\newpage
### 6. Strange Stories: Partial successes

Unlike other tasks, Strange Stories uses a three-level scoring system rather than a binary correct/incorrect judgement. As such, while the session-level responses of other tasks can be inferred from their aggregated scores, the Strange Stories have two ways that responses can lose points: Responses that fail to understand or explain the story in a meaningful way are coded as failures, while explaining the events of a story in non-mentalistic terms are rated as partial successes. Only explanations that describe the relevant mental states of the characters are rated as full successes. As an example, consider the following story: 

_Simon is a big liar. Simon's brother Jim knows this, he knows that Simon never tells the truth! Now yesterday Simon stole Jim's ping-pong paddle, and Jim knows Simon has hidden it somewhere, though he can't find it. He's very cross. So he finds Simon and he says, "Where is my ping-pong paddle? You must have hidden it either in the cupboard or under your bed, because I've looked everywhere else. Where is it, in the cupboard or under your bed"? Simon tells him the paddle is under his bed. Why will Jim look in the cupboard for the paddle?_

Examples of each kind of answer:

* Failure: _Jim will not look in the cupboard for the paddle because Simon has told him that the paddle is under his bed._

* Partial Success: _Jim will look in the cupboard for the paddle because Simon lied about where it was hidden, claiming that it was under his bed when it was actually somewhere else. Therefore, Jim cannot trust Simon's answer about where he hid the paddle and needs to check both places to find it._ [This is only a partial success because it does not recognise that Jim will use his knowledge of Simon's untrustworthiness to reason about where the paddle actually is]

* Full Success: _Jim will look in the cupboard for the paddle because he knows that Simon is a big liar and never tells the truth. Since Simon said the paddle is under his bed, Jim believes the opposite must be true, so he will look in the cupboard instead._

As shown in Figure S7, breaking down different response types revealed that partial successes were infrequent, and were more likely for humans and GPT-3.5 than for GPT-4, and more likely for LLaMA2-70B than all other models.

```{r FigS7, out.width='60%', fig.width = 5, fig.height = 4, fig.cap="Counts of different response types on the Strange Stories test: failure, mixed success, and full success."}
ss_llm <- df %>%
    filter(source == 'old') %>%     # get old items
    gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
    mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>% 
    filter(!is.na(score), task == 'Strange Stories') %>%
    group_by(task, model,score) %>% summarise(count = n()/120) %>% ungroup() %>% add_row(task = 'Strange Stories', model = 'GPT-4',score = 0, count = 0) %>%
    mutate(score = factor(score, levels = c(0, 0.5, 1), labels = c('Failure','Partial success','Success'))) 

ss_hum <- dfh %>%
    filter(source == 'old') %>%     # get old items
    gather(trial, score, -task, -item, -source, -trial_state, -model) %>% 
    mutate(trial = factor(trial, levels = sprintf('score%s',c(1:15)))) %>% 
    filter(!is.na(score), task == 'Strange Stories') %>%
    mutate(model = 'Human') %>% 
    group_by(task, model,score) %>% summarise(count = n()/nrow(.)) %>% ungroup() %>% 
    mutate(score = factor(score, levels = c(0, 0.5, 1), labels = c('Failure','Partial success','Success'))) 

fs9_sstories <- rbind(ss_llm,ss_hum) %>% 
  mutate(model = factor(model, 
                        levels = c('Human','GPT-4','GPT-3.5','LLaMA-70B'),
                        labels = c('Human','GPT-4','GPT-3.5','LLaMA2-70B'))) %>% 
    ggplot(aes(x = model, y = count, group = score, fill = score)) + 
    geom_bar(stat = 'identity', alpha = .5, position = 'dodge') +
    geom_point(position = position_dodge(width = 0.9), colour = 'black') + 
    theme(legend.title = element_blank(), legend.position = 'bottom') + facet_grid(.~task) + xlab('Model') + ylab('Proportion') +
    scale_fill_manual(values = createColourValues(cp$StrangeStories, n = 3, rev = T)) + th + 
    theme(legend.position = 'bottom')

print(fs9_sstories)
ggsave(fs9_sstories, filename = 'Figures/FigureS7.png', width = 5, height = 4, dpi = 300)
```

\newpage
### 7. Qualitative Analysis of Faux Pas Likelihood test

The Faux Pas Likelihood test, which presented stories with a final question framed in terms of likelihood rather than an objective property of the character’s mind, resulted in much better performance of GPT models than the original framing of the question. However, while both models scored very high, explanations spontaneously added by GPT models suggest that successes did not all follow from correct inference. To quantify this intuition, we refined the coding criteria to differentiate between different subtypes of successes, listed below with examples of possible responses fitting the criteria:

* **Full explanation** - The response provides an explanation that goes beyond the limits of what is described in the story to infer the cause of an offensive utterance using hypothetical reasoning or counterfactuals. Example: _"It is more likely that Alice did not realize that Emma hadn't heard the results of the competition. If she had realized, she would have been more careful not to reveal the outcome of the competition inadvertently."_

* **Event explanation** - The response provides an explanation that provides a rational or adequate explanation, but it does so by drawing only on information included within the story without or with only minimal additional inference. Example: _"It is more likely that Becky did not know that Simon was sick before making her comment, as she arrived late to class after the news was announced.It is more likely that Becky did not know that Simon was sick, as she arrived late to class and likely did not hear the teacher's announcement about Simon's illness before making her insensitive comment."_

* **No explanation** - The response provides the correct answer (that it is more likely they the speaker did not know) without any additional explanation. Alternatively, the response may reproduce details from the story that do not actually explain the behaviour. Example: _"It is less likely that Richard remembered that James had given him the toy aeroplane for his birthday, as he made a comment that suggests he did not appreciate the gift, even though it was given to him by James."_

* **Tautological explanation** - The response provides the correct answer but with an explanation that assumes the conclusion it is trying to explain. Example: _"It is more likely that Lisa did not know that the curtains were new, as she made the insensitive comment without realizing that Jill had just purchased them."_

We also coded mixed successes and failures according to different subtypes:

* **Mixed success following an uncertain response** - The first response provided was uncertain or non-committal (e.g. 'It is not clear...') but the model provided the correct answer when prompted to provide the most likely explanation 

* **Mixed success following an incorrect response** - In the initial response the model correctly identified that something inappropriate was said but claimed that the speaker understood the context, leaving an open question as to why they said what they said. Following a follow-up prompt, the model provided an explanation that gave some acknowledgement that the offense was nonetheless unintentional 

* **Failure to recognise the offense** - The response does not report that the key statement was offensive or inappropriate, meaning that a follow-up to understand why the speaker might have said it is unnecessary

* **Failure with recognition of offense** - After being prompted (either following an uncertain or incorrect response), the model still does not provide the correct explanation for the faux pas 

The counts of these different kinds of responses are shown in Figure S8. As shown in Figure S8A, the pattern of response for successes was similar for GPT-4 and GPT-3.5. Full and complete explanations involving hypotheticals or subjunctive clauses were rare: more often, the models would provide explanations that restipulated the events or facts related in the narrative. The most frequent elaboration for both models, however, was to present tautologies or circular descriptions as though they were explanations. 

Mixed successes and failures were rare and exclusively seen in responses from GPT-3.5, the most common type of failure being failure to recognise a statement as offensive. 

```{r}
# load data from follow-up 
df_fp_fu <- read.csv('scored_data/scores_followup.csv', header = T)

# arrange data into a dataframe. The qualitative outcome types are not relevant for the main analysis but are described in the Supplementary Material Section S3
df_fp2 <- df_fp_fu %>% 
  gather(trial, type, -task, -item, -source, -trial_state, -model) %>% 
  mutate(outcome = factor(ifelse(type %in% c('A*','A+','A','A-','D'), 'Success',
                        ifelse(type %in% c('B','C'), 'Mixed success',
                               ifelse(type %in% c('E','F'),'Failure',NA))),
                        levels = c('Success','Mixed success','Failure',NA)),
         score = (3-as.numeric(outcome))/2) %>%
  filter(!is.na(score))

df_fp2_s <- df_fp2 %>% mutate(
  outcome = factor(ifelse(type %in% c('A*','A+','A','A-'), 'Success',
                        ifelse(type %in% c('B','C'), 'Mixed success',
                               ifelse(type %in% c('D','E','F'),'Failure',NA))),
                        levels = c('Success','Mixed success','Failure',NA)),
  outcome_spec = factor(
    type,
    levels = c('A*','A+','A','A-','B','C','D','E','F'),
    labels = c('Full explanation', 'Event explanation','No explanation','Tautological explanation',
               'Original Uncertain','Original Incorrect',
               'Unrecognised offence', 'Unrecognised offence','Incorrect after prompt'))
)
```

```{r FigS8, fig.width = 10, fig.height = 6, out.width='80%', fig.cap="Qualitative breakdown of response types on Faux Pas Likelihood test. Barplots showing counts of the different response types on the adapted Faux Pas Likelihood test. The figures show the number of four identified types of successes, two types of mixed successes, and two types of failures identified through manual coding of the responses"}
lg_size <- 0.25

cp2_success <- createColourValues(cp$success, n = 4)
cp2_mixedsc <- createColourValues(cp$mixed, n = 2)
cp2_failure <- createColourValues(cp$failure, n = 2)

fig_sc <- df_fp2_s %>% 
    filter(outcome == 'Success') %>%
    ggplot(aes(x = model, fill = outcome_spec)) + 
    geom_bar(position = position_dodge()) +
    geom_text(position = position_dodge(width=0.9), stat = 'count', aes(label = ..count..), vjust = 1.5) +
    scale_fill_manual(name = 'Successes', values = cp2_success) + 
    th + theme(legend.position = 'right', 
               legend.title = element_text(face = 'bold', size = fs),
               legend.text = text,
               legend.key.size = unit(lg_size, 'cm')) +
    xlab('Model') + ylab('Count') 

fig_ms <- df_fp2_s %>% 
    filter(outcome == 'Mixed success') %>%
    ggplot(aes(x = model, fill = outcome_spec)) + 
    geom_bar(position = position_dodge()) +
    geom_text(position = position_dodge(width=0.9), stat = 'count', aes(label = ..count..), vjust = 1.5) +
    scale_fill_manual(name = 'Mixed', values = cp2_mixedsc) + 
    th + theme(legend.position = 'right', 
               legend.title = element_text(face = 'bold', size = fs),
               legend.text = text,
               legend.key.size = unit(lg_size, 'cm')) +
    xlab('Model') + ylab('Count') 

fig_fl <- df_fp2_s %>% 
    filter(outcome == 'Failure') %>%
    ggplot(aes(x = model, fill = outcome_spec)) + 
    geom_bar(position = position_dodge()) +
    geom_text(position = position_dodge(width=0.9), stat = 'count', aes(label = ..count..), vjust = 1.5) +
    scale_fill_manual(name = 'Failures', values = cp2_failure) + 
    th + theme(legend.position = 'right', 
               legend.title = element_text(face = 'bold', size = fs),
               legend.text = text,
               legend.key.size = unit(lg_size, 'cm')) +
    xlab('Model') + ylab('Count')

fs8_stacked <- ggarrange(
  fig_sc + theme(legend.position = 'bottom') + guides(fill = guide_legend(nrow = 2)),
  fig_ms + theme(legend.position = 'bottom') + guides(fill = guide_legend(nrow = 2)),
  fig_fl + theme(legend.position = 'bottom') + guides(fill = guide_legend(nrow = 2)), ncol = 3, widths = c(1.5,0.8,0.8), labels = c('A','B','C'))

print(fs8_stacked)
ggsave(fs8_stacked, filename = 'Figures/FigureS8.png', width = 12, height = 5, dpi = 300)
```
### Appendix 1. Full text of False Belief Perturbations adapted from Ullman (2023)

```{r, include=F}
fb_table <- 
  tibble(Condition = c('False Belief','Transparent','Preposition','Testimony','Mover'),
         Story1 = c(
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something.\n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn’t know what happened in the room when he was away.\n\nWhen John comes back home, where will he look for the cat?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something.\n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story \n\n\n\nIn the room there are John, Mark, a cat, **a transparent plastic box**, and a **glass chest**. John takes the cat and puts it in the **chest**. He leaves the room and goes to school. While John is away, Mark takes the cat out of the **chest** and puts it in the **box**. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn’t know what happened in the room when he was away.\n\nWhen John comes back home, where will he look for the cat?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something.\n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story \n\n\n\nIn the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it **on** the basket. He leaves the room and goes to school. While John is away, Mark takes the cat off the basket and puts it **on** the box. Mark leaves the room and goes to work. John comes back from school and enters the room. **John looks around the room.** He doesn’t know what happened in the room when he was away\n\nWhen John comes back home, where will he look for the cat?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. **Mark calls John to tell him he is going to move the cat to the box. John believes him.** While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. John comes back from school and enters the room. He doesn’t know what happened in the room when he was away.\n\nWhen John comes back home, where will he look for the cat?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the room there are John, Mark, a cat, a box, and a basket. John takes the cat and puts it in the basket. He leaves the room and goes to school. While John is away, Mark takes the cat out of the basket and puts it in the box. Mark leaves the room and goes to work. **John and Mark** come back and enter the room. **They** don’t know what happened in the room when they were away.\n\nWhen **Mark** comes back home, where will he look for the cat?")
         ),
         Story2 = c(
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something.\n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the kitchen there are Lucy, Mia, a carton of orange juice, a fridge, and a cupboard. Lucy puts the carton of orange juice in the cupboard. She then leaves the kitchen and goes to school. While Lucy is away, Mia takes the orange juice out of the cupboard and puts it in the fridge. Mia leaves the room and goes to work. Lucy comes back from school and enters the kitchen. She doesn’t know what happened in the kitchen when she was away. \n\nWhen Lucy comes back home, where will she look for the orange juice?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the kitchen there are Lucy, Mia, a carton of orange juice, a **transparent plastic box**, and a **glass-fronted cabinet** Lucy puts the carton of orange juice in the **transparent plastic box**. She then leaves the kitchen and goes to school. While Lucy is away, Mia takes the orange juice out of the **box** and puts it in the **glass-fronted cabinet**. Mia leaves the room and goes to work. Lucy comes back from school and enters the kitchen. She doesn’t know what happened in the kitchen when she was away. \n\nWhen Lucy comes back home, where will she look for the orange juice?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the kitchen there are Lucy, Mia, a carton of orange juice, a fridge, and a cupboard. Lucy puts the carton of orange juice **on** the cupboard. She then leaves the kitchen and goes to school. While Lucy is away, Mia takes the orange juice **off of** the cupboard and puts it **on** the fridge. Mia leaves the room and goes to work. Lucy comes back from school and enters the kitchen. **Lucy looks around the kitchen.** She doesn’t know what happened in the kitchen when she was away. \n\nWhen Lucy comes back home, where will she look for the orange juice?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the kitchen there are Lucy, Mia, a carton of orange juice, a fridge, and a cupboard. Lucy puts the carton of orange juice in the cupboard. She then leaves the kitchen and goes to school. **Mia texts Lucy to tell her that she is going to move the orange juice to the fridge. Lucy believes her.** While Lucy is away, Mia takes the orange juice out of the cupboard and puts it in the fridge. Mia leaves the room and goes to work. Lucy comes back from school and enters the kitchen. She doesn’t know what happened in the kitchen when she was away. \n\nWhen Lucy comes back home, where will she look for the orange juice?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the kitchen there are Lucy, Mia, a carton of orange juice, a fridge, and a cupboard. Lucy puts the carton of orange juice in the cupboard. She then leaves the kitchen and goes to school. While Lucy is away, Mia takes the orange juice out of the cupboard and puts it in the fridge. Mia leaves the room and goes to work. **Lucy and Mia** comes back from school and enters the kitchen. **They** don’t know what happened in the kitchen when they were away. \n\nWhen **Mia** comes back home, where will she look for the orange juice?")
         ),
         Story3 = c(
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the living room there are Grace, her grandmother, some chocolate biscuits, a metal tin, and a ceramic jar. Whenever Grace visits her grandmother, she always gets a chocolate biscuit from where they are stored in the metal tin. Today, she gets a biscuit and then leaves. While Grace is gone, her grandmother takes the chocolate biscuits out of the metal tin and puts them into the ceramic jar. Grace comes back for a visit and enters the living room. She doesn’t know what happened in the living room when she was away. \n\nWhen Grace comes to visit, where will she look for the chocolate biscuits?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the living room there are Grace, her grandmother, some chocolate biscuits, a **clear plastic container**, and a **glass jar**. Whenever Grace visits her grandmother, she always gets a chocolate biscuit from where they are stored in the **clear plastic container**. Today, she gets a biscuit and then leaves. While Grace is gone, her grandmother takes the chocolate biscuits out of the **clear plastic container** and puts them into the **glass jar**. Grace comes back for a visit and enters the living room. She doesn’t know what happened in the living room when she was away. \n\nWhen Grace comes to visit, where will she look for the chocolate biscuits?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the living room there are Grace, her grandmother, some chocolate biscuits, **a metal tray**, and **a ceramic plate**. Whenever Grace visits her grandmother, she always gets a chocolate biscuit from where they are stored **on the metal tray**. Today, she gets a biscuit and then leaves. While Grace is gone, her grandmother takes the chocolate biscuits **off of the metal tray** and puts them **onto the ceramic plate**. Grace comes back for a visit and enters the living room. **Grace looks around the living room.** She doesn’t know what happened in the living room when she was away. \n\nWhen Grace comes to visit, where will she look for the chocolate biscuits?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the living room there are Grace, her grandmother, some chocolate biscuits, a metal tin, and a ceramic jar. Whenever Grace visits her grandmother, she always gets a chocolate biscuit from the metal tin. Today, **she gets a biscuit and her grandmother tells her that she is going to move the chocolate biscuits from the metal tin to the ceramic jar before Grace next visits. Grace believes her grandmother, and she leaves.** While Grace is gone, her grandmother takes the chocolate biscuits out of the metal tin and puts them into the ceramic jar. Grace comes back for a visit and enters the living room. She doesn’t know what happened in the living room when she was away. \n\nWhen Grace comes to visit, where will she look for the chocolate biscuits?"),
           sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\nWhen I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\nIn the living room there are Grace, her grandmother, some chocolate biscuits, a metal tin, and a ceramic jar. Whenever Grace visits her grandmother, she always gets a chocolate biscuit from where they are stored in the metal tin. Today, she gets a biscuit and then leaves. While Grace is gone, her grandmother takes the chocolate biscuits out of the metal tin and puts them into the ceramic jar. The grandmother then leaves to go shopping. **Grace and her grandmother come back and enter the living room. They** don’t know what happened in the living room when they were away. \n\nWhen **the grandmother** comes back home, where will she look for the chocolate biscuits?")
         ))
```

```{r falsebelief_table}
kable(fb_table,col.names = c('Variant',"John's cat","Lucy's juice","Grace's biscuits")) %>% kable_styling()
```

### Appendix 2. Full Text of items generated for the Belief Likelihood Test 

```{r, include=F}
fp_table <- tibble(
  Story = c(1:6),
  FauxPas = c(
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Michael was a very awkward child when he was at high school. He struggled with making friends and spent his time alone writing poetry. However, after he left he became a lot more confident and sociable. At his ten-year high school reunion he met Amanda, who had been in his English class. Over drinks, she said to him, '**I don't know if you remember this guy from school. He was in my English class. He wrote poetry and he was super awkward. I hope he isn't here tonight.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere were Michael and Amanda? \n\nIs it more likely that Amanda knew or didn't know that Michael was the boy from her English class?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Laura painted a picture of Olivia, who decided to hang it in her living room at home. A couple of months later, Olivia invited Laura to her place. While the two friends chatted over a cup of tea in the living room, Olivia’s son came in and said, '**Laura, you should help my mum choose which paintings to hang in the house, as you can see she has no good taste at all!**' \n\n In the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Olivia hang Laura’s painting? \n\nIs it more likely that Olivia’s son knew or didn't know that Laura painted the painting?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jeremy had been saving up for months to buy his dream car: a convertible sports car that was painted a very vivid green. When he finally bought it he drove to work early and parked it in the best parking spot directly in front of his office. Later, his colleague Sophie arrived and said, '**There must be someone very rich visiting today because the ugliest car I’ve ever seen is parked out front!**' \n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere was Jeremy’s car parked?\n\nIs it more likely that Sophie knew or didn't know that the car was Jeremy’s?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jim was happy because his parents just opened a new restaurant near where he worked. One day, at work, he decided to invite his colleagues to the restaurant. His colleague Ann said, '**I’d love to go out to dinner, but I strongly advise against that restaurant, I went the other night, and the food was terrible.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Jim invite his colleagues? \n\nIs it more likely that Ann knew or didn't know that the new restaurant belonged to Jim’s parents?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jane was talking to her sister on Christmas Eve about how her gift for their brother Matt: a big box of sweets from a recent vacation. That evening around the dinner table, Matt said, '**I’m looking forward to exchanging presents tomorrow. I just hope nobody got something unimaginative like sweets!**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhat gift had Jane bought her brother for Christmas?\n\nIs it more likely that Matt knew or didn't know that Jane had bought him sweets?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Gareth was the singer in a band who had one well-known hit decades ago but no further success. He went into a café where Emma was working. “Good morning, how can I help you today?” asked Emma. Gareth was about to reply when his song came on the radio. Emma quickly turned the radio off and said, '**Not that song again. I hate it.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWho was working at the café?\n\nIs it more likely that Emma knew or didn't know that Gareth wrote the song on the radio?")
  ),
  Neutral = c(
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I've finished telling it I will ask you some questions about what happened in the story\n\n\n\n Michael was a very awkward child when he was at high school. He struggled with making friends and spent his time alone writing poetry. However, after he left he became a lot more confident and sociable. At his ten-year high school reunion he met Amanda, who had been in his English class. Over drinks, she said to him, '**Do you know where the bar is?**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere were Michael and Amanda? \n\nIs it more likely that Amanda knew or didn't know that Michael was the boy from her English class?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I've finished telling it I will ask you some questions about what happened in the story\n\n\n\n Laura painted a picture of Olivia, who decided to hang it in her living room at home. A couple of months later, Olivia invited Laura to her place. While the two friends chatted over a cup of tea in the living room, Olivia’s son came in and said, '**I'm looking forward to your parter next week, Laura.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Olivia hang Laura’s painting? \n\nIs it more likely that Olivia’s son knew or didn't know that Laura painted the painting?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jeremy had been saving up for months to buy his dream car: a convertible sports car that was painted a very vivid green. When he finally bought it he drove to work early and parked it in the best parking spot directly in front of his office. Later, his colleague Sophie arrived and said, '**My alarm clock broke this morning. I need to buy a new one after work.**' \n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere was Jeremy’s car parked?\n\nIs it more likely that Sophie knew or didn't know that the car was Jeremy’s?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jim was happy because his parents just opened a new restaurant near where he worked. One day, at work, he decided to invite his colleagues to the restaurant. His colleague Ann said, '**I'm sorry, but I can't go tonight. I'm going to the cinema with my sister.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Jim invite his colleagues? \n\nIs it more likely that Ann knew or didn't know that the new restaurant belonged to Jim’s parents?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jane was talking to her sister on Christmas Eve about how her gift for their brother Matt: a big box of sweets from a recent vacation. That evening around the dinner table, Matt said, '**I hope everybody remembered their Christmas jumpers for tomorrow.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhat gift had Jane bought her brother for Christmas?\n\nIs it more likely that Matt knew or didn't know that Jane had bought him sweets?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Gareth was the singer in a band who had one well-known hit decades ago but no further success. He went into a café where Emma was working. “Good morning, how can I help you today?” asked Emma. Gareth was about to reply when his song came on the radio. Emma quickly turned the radio off and said, '**We have a special deal on if you want a pastry with any cold drink.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWho was working at the café?\n\nIs it more likely that Emma knew or didn't know that Gareth wrote the song on the radio?")
  ),
  KnowledgeImplied = c(
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Michael was a very awkward child when he was at high school. He struggled with making friends and spent his time alone writing poetry. However, after he left he became a lot more confident and sociable. At his ten-year high school reunion he met Amanda, who had been in his English class. Over drinks, she said to him, '**Do you still write poetry?**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere were Michael and Amanda? \n\nIs it more likely that Amanda knew or didn't know that Michael was the boy from her English class?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Laura painted a picture of Olivia, who decided to hang it in her living room at home. A couple of months later, Olivia invited Laura to her place. While the two friends chatted over a cup of tea in the living room, Olivia’s son came in and said, '**I'd love to have a portrait of myself to hang in my room.**' \n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Olivia hang Laura’s painting? \n\nIs it more likely that Olivia’s son knew or didn't know that Laura painted the painting?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jeremy had been saving up for months to buy his dream car: a convertible sports car that was painted a very vivid green. When he finally bought it he drove to work early and parked it in the best parking spot directly in front of his office. Later, his colleague Sophie arrived and said, '**Hey. I'd love to go for a ride after work.”**' \n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere was Jeremy’s car parked?\n\nIs it more likely that Sophie knew or didn't know that the car was Jeremy’s?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jim was happy because his parents just opened a new restaurant near where he worked. One day, at work, he decided to invite his colleagues to the restaurant. His colleague Ann said, '**Oh I'm not sure. I don't want your parents to assume we're dating.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhere did Jim invite his colleagues? \n\nIs it more likely that Ann knew or didn't know that the new restaurant belonged to Jim’s parents?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Jane was talking to her sister on Christmas Eve about how her gift for their brother Matt: a big box of sweets from a recent vacation. That evening around the dinner table, Matt said, '**I’m looking forward to exchanging presents tomorrow. I can't wait to indulge my sweet tooth.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWhat gift had Jane bought her brother for Christmas?\n\nIs it more likely that Matt knew or didn't know that Jane had bought him sweets?"),
    sprintf("I am going to tell you a short story about some people. At the end of this story a person will say or do something. \n\n When I’ve finished telling it I will ask you some questions about what happened in the story\n\n\n\n Gareth was the singer in a band who had one well-known hit decades ago but no further success. He went into a café where Emma was working. “Good morning, how can I help you today?” asked Emma. Gareth was about to reply when his song came on the radio. Emma quickly turned the radio off and said, '**Oh, what funny timing! I love this song.**'\n\nIn the story did someone say something that they should not have said? \n\nWhat did they say that they should not have said? \n\nWho was working at the café?\n\nIs it more likely that Emma knew or didn't know that Gareth wrote the song on the radio?")
  )
)
```

```{r fauxpas_table}
kable(fp_table,col.names = c('Story','Faux Pas','Neutral','Knowledge Implied')) %>% kable_styling()
```

## Generate Figures {.unnumbered}
<h2>Generate Figures</h2>

### Figure 1

Figure 1: Main effects

* Violin plot showing performance of both models on original items for each test

* Radar plot showing model performance against human baseline

* Control analysis: original vs. new

* Faux Pas: New question

```{r, fig.height=8, fig.width=8, warning = F, out.width='100%'}
fig1 <- ggarrange(
    f1A_violin + xlab('') + 
        theme(axis.text.x = element_text(size = fs, face = 'bold', vjust = 0.5,
                                         margin = margin(t = 8, b = 0,unit = 'pt')),
              plot.margin = margin(b=0,unit = 'pt')) + 
        scale_x_discrete(expand = c(-0.18,1.18)),
    f1B_barbell + theme(axis.text.y = element_text(size = fs_s), strip.text.x = element_blank(),
              plot.margin = margin(t=0,unit = 'pt')), common.legend = T,
    labels = c('A','B'), nrow = 2, heights = c(1.2,1))
    
print(fig1)
```

### Figure 2

Figure 2: Results of new variants 

* Results of control variants for all models in False Belief 

* Results of likelihood framing for GPT3.5 and GPT4

* Results of control variants for all models in Faux Pas

```{r, fig.height=4, fig.width=8.5, warning = F, out.width='100%'}
fig2 <- ggarrange(
        f2A_FP2 + theme(plot.margin = margin(5.5,5.5,5.5,40,unit = 'pt'), legend.text = text),
        f2B_fpv + theme(plot.margin = margin(5.5,5.5,15,5.5, 'pt'), legend.text = text),
        labels = c('A','B'), ncol = 2, widths = c(1,1.1))

print(fig2)
```

### Save figures
```{r}
ggsave('Figures/Figure1.png',
      fig1,
      width = 18, height = 18, units = 'cm', dpi = 300)

ggsave('Figures/Figure2.png',
      fig2,
      width = 25, height = 10, units = 'cm', dpi = 300)
```

